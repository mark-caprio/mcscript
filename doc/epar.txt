Scratch code for embarassingly parallel serial runs

11/22/16 (mac): extracted from legacy mcscript

Old embarassingly parallel implementation code:


        # old code for local definitions

        ## if (self.batch_mode):
        ##     if (self.parallel_epar != -1):
        ##         # set configuration so serial codes will launch locally under this
        ##         # daughter process (presumably already on compute node)
        ##         self.serial_prefix = local.serial_prefix("epar")
        ##     else:
        ##         # set configuration so serial codes will launch appropriately for
        ##         # batch jobs at the present facility (perhaps on a separate
        ##         # compute node if the job scripts run on dedicated nodes distinct
        ##         # from the compute nodes)
        ##         self.serial_prefix = local.serial_prefix("batch")
        ## else:
        ##     # local run on front end
        ##     self.serial_prefix = local.serial_prefix("local")
        ## self.parallel_prefix = local.parallel_prefix(self)

        # epar rank
        ## if (self.parallel_epar != -1):
        ##     # load MPI machinery to determine rank
        ##     from mpi4py import MPI
        ##     comm = MPI.COMM_WORLD
        ##     self.epar_rank = comm.Get_rank()
        ## else:
        ##     self.epar_rank = None

In init:            

    ## ################################################################
    ## # handle embarassingly parallel relaunch -- SUPPLANTED
    ## ################################################################
    ## 
    ## if (parallel_epar_status == "parent"):
    ##     # launch epar daughters
    ##     print "Embarassingly parallel mode (launching daughters)..." 
    ##     sys.stdout.flush()   # needed so output doesn't get mixed with daughters
    ##     os.environ["QSUBM_EPAR"] = "daughter"
    ##     subprocess.call(mcscript.local.epar_args)
    ##     sys.stdout.flush()
    ##     print "Daughters returned..."
    ##     exit

    ################################################################
    # epar logging
    ################################################################

    if (run.parallel_epar != -1):

        # trap invalid epar situation
        if (not run.batch_mode):
            raise ValueError("parallel_epar: Cannot have embarassingly parallel mode in local run.")

        # flag embarassingly parallel daughter process
        print("Embarassingly parallel mode...")
        print("My rank is %d..." % run.epar_rank)  # TODO fix to work now that epar_rank not stored in run
        sys.stdout.flush()



    if (self.batch_mode):
        if (self.parallel_epar != -1):
            # set configuration so serial codes will launch locally under this
            # daughter process (presumably already on compute node)
            serial_mode = "epar"
        else:
            # set configuration so serial codes will launch appropriately for
            # batch jobs at the present facility (perhaps on a separate
            # compute node if the job scripts run on dedicated nodes distinct
            # from the compute nodes)
            serial_mode = "batch"
    else:
        # local run on front end
        serial_mode = "local"



ndcrc submission:

    # job command
    if (args.epar is not None):
        # embarassingly parallel serial job
        # TODO -- support smp on embarassingly parallel run
        submission_invocation += [
            os.path.join(qsubm_path,"wrapper.csh"),     # csh wrapper
            "mpiexec", # parallel relaunch
            "-n%d" % args.epar,
            ## "-b", "/usr/bin/env",  # executable wrapper to launch on compute node -- see if needed at NDCRC?
            "python", # call interpreter, so py file does not need to be executable
            job_file
            ]
    else:
        # plain non-epar job 
        submission_invocation += [
            os.path.join(qsubm_path,"wrapper.csh"),  # csh wrapper required at NDCRC
            "python", # call interpreter, so py file does not need to be executable
            job_file
            ]



            "Epar: {}".format(self.parallel_epar)
